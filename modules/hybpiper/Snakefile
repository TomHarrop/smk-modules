#!/usr/bin/env python3

import re
import tempfile
from snakemake.logging import logger
from functools import cache

#############
# FUNCTIONS #
#############


@cache
def sample_name_sanitiser(sample_name):
    if re.compile("[^a-zA-Z0-9_-]").search(sample_name):
        raise ValueError(f"{sample_name} contains special character(s)")


@cache
def read_namelist(namelist):
    with open(namelist, "rt") as f:
        return sorted(set(x.rstrip("\n") for x in f.readlines()))


def get_sample_list(wildcards):
    namelist_file = checkpoints.collect_namelist.get(**wildcards).output[0]
    return read_namelist(namelist_file)


# NOTE. This function will check for completed assembly files WHEN THE WORKFLOW
# STARTS. It will not find them if they are produced after startup.
def get_sample_files(wildcards):
    sample_paths = []
    for sample in get_sample_list(wildcards):
        if Path(outdir, "assemble", f"{sample}.tar.gz").resolve().is_file():
            sample_paths.append(Path("assemble", f"{sample}.tar.gz"))
        else:
            sample_paths.append(Path("assemble", f"{sample}.tar"))
    return [Path(outdir, x) for x in sample_paths]


def get_assembly_archives(wildcards):
    sample_paths = expand(
        Path("assemble", "{sample}.tar.gz"),
        sample=get_sample_list(wildcards),
    )
    return [Path(outdir, x) for x in sample_paths]


###########
# GLOBALS #
###########

# containers
# NOTE! Hybpiper 2.1.8--pyhdfd78af_0 fails with NameError: name
# 'MafftCommandline' is not defined
hybpiper = "docker://quay.io/biocontainers/hybpiper:2.1.7--pyhdfd78af_0"
pigz = "docker://quay.io/biocontainers/pigz:2.3.4"
bbmap = "docker://quay.io/biocontainers/bbmap:39.01--h92535d8_1"

# set up directories
outdir = Path(config["outdir"] if "outdir" in config else ".")
logger.debug(f"outdir: {outdir}")
logdir = Path(outdir, "logs")

# catch samples and
namelist = config["namelist"]
logger.debug(f"Caught namelist: {namelist}")


# This was to make sure nothing crazy happens with sample names. But we can't
# do this if we are reading the namelist as a checkpoint.
# wildcard_constraints:
#     sample="|".join(read_samples_from_namelist(namelist)),
# Instead, use the allowed characters a la tcdemux
wildcard_constraints:
    sample="[a-zA-Z0-9_-]+",


# catch read directory
read_directory = config["read_directory"]
logger.debug(f"Caught read_directory: {read_directory}")

# catch target_file
target_file = config["target_file"]
logger.debug(f"Caught target_file: {target_file}")

tmp_target_file = Path(outdir, "targetfile.fasta")
fixed_tf = Path(outdir, "target_file.fixed.fasta")


# This code block untars the sample files in input.sample_files into the cwd so
# that hybpiper can find them using the namelist. Works with "shallow:
# minimal".
untar_sample_folders = (
    "( "
    "parallel -j {threads} "
    "./{input.untar_script} "
    "::: {input.sample_files} "
    ") &> {log} ; "
)


############
# WORKFLOW #
############

hybpiper_outputs = [
    Path(outdir, "sequences.dna.tar.gz"),
    Path(outdir, "sequences.aa.tar.gz"),
    Path(outdir, "sequences.intron.tar.gz"),
    Path(outdir, "sequences.supercontig.tar.gz"),
    Path(outdir, "hybpiper_stats.gene.tsv"),
    Path(outdir, "hybpiper_stats.supercontig.tsv"),
    Path(outdir, "seq_lengths.gene.tsv"),
    Path(outdir, "seq_lengths.supercontig.tsv"),
    Path(outdir, "paralogs.all.tar.gz"),
    Path(outdir, "paralogs.no_chimeras.tar.gz"),
]


rule target:
    input:
        hybpiper_outputs,
        get_assembly_archives,


# Need lots of files here, try to do it in the job tmp area provided to
# Snakemake (not the individual jobs). This needs to be passed as a config
# parameter to the call to snakemake, because TMPDIRs are interpreted at
# runtime. This would mean that worker nodes get a different run_tmpdir if
# tempfile.mkdtemp() is used in the Snakefile.
rule archive_results:
    input:
        folder=Path(outdir, "hybpiper_results", "{result}.{type}"),
    output:
        tarfile=Path(outdir, "{result}.{type}.tar.gz"),
    shadow:
        "minimal"
    threads: lambda wildcards, attempt: 4 * attempt
    resources:
        time=lambda wildcards, attempt: 20 * attempt,
    container:
        pigz
    shell:
        "tar -cf - --directory={input.folder} . "
        "| pigz -p {threads} -9 "
        "> {output.tarfile} "


rule retrieve_paralogs:
    input:
        target=fixed_tf,
        namelist=Path(outdir, "namelist.txt"),
        sample_files=get_sample_files,
        untar_script=Path(outdir, "untar.sh"),
    output:
        all_dir=temp(
            directory(Path(outdir, "hybpiper_results", "paralogs.all"))
        ),
        nochimeras_dir=temp(
            directory(
                Path(outdir, "hybpiper_results", "paralogs.no_chimeras")
            ),
        ),
        paralog_report=Path(outdir, "paralog_report.tsv"),
        paralogs_above_threshold=Path(outdir, "paralogs_above_threshold.txt"),
        heatmap=Path(outdir, "paralog_heatmap.png"),
    shadow:
        "minimal"
    params:
        parent=lambda wildcards, input: Path(input.sample_files[0]).parent,
    log:
        Path(logdir, "retrieve_paralogs.log"),
    benchmark:
        Path(logdir, "retrieve_paralogs.benchmark.log")
    threads: lambda wildcards, attempt: 16 * attempt
    resources:
        time=lambda wildcards, attempt: 120 * attempt,
    container:
        hybpiper
    shell:
        f"{untar_sample_folders} "
        "hybpiper paralog_retriever "
        "--targetfile_dna {input.target} "
        "--fasta_dir_all paralogs.all "
        "--fasta_dir_no_chimeras paralogs.no_chimeras "
        "--paralog_report_filename paralog_report "
        "--paralogs_above_threshold_report_filename paralogs_above_threshold "
        "{input.namelist} "
        "&> {log} "
        "&& "
        "mv paralog_report.tsv "
        "{output.paralog_report} "
        "&& "
        "mv paralogs_above_threshold.txt "
        "{output.paralogs_above_threshold} "
        "&& "
        "mv paralog_heatmap.png "
        "{output.heatmap} "
        "&& "
        "mv paralogs.no_chimeras "
        "{output.nochimeras_dir} "
        "&& "
        "mv paralogs.all "
        "{output.all_dir} "


rule retrieve_sequences:
    input:
        target=fixed_tf,
        namelist=Path(outdir, "namelist.txt"),
        sample_files=get_sample_files,
        untar_script=Path(outdir, "untar.sh"),
    output:
        fasta_dir=temp(
            directory(Path(outdir, "hybpiper_results", "sequences.{type}"))
        ),
    shadow:
        "minimal"
    params:
        parent=lambda wildcards, input: Path(input.sample_files[0]).parent,
    log:
        Path(logdir, "retrieve_sequences.{type}.log"),
    benchmark:
        Path(logdir, "retrieve_sequences.{type}.benchmark.log")
    threads: lambda wildcards, attempt: 16 * attempt
    resources:
        time=lambda wildcards, attempt: 120 * attempt,
    container:
        hybpiper
    shell:
        f"{untar_sample_folders} "
        "hybpiper retrieve_sequences "
        "--targetfile_dna {input.target} "
        "--sample_names {input.namelist} "
        "--fasta_dir sequences.{wildcards.type} "
        "{wildcards.type} "
        "&> {log} "
        "&& mv sequences.{wildcards.type} {output.fasta_dir} "


rule stats:
    input:
        target=fixed_tf,
        namelist=Path(outdir, "namelist.txt"),
        sample_files=get_sample_files,
        untar_script=Path(outdir, "untar.sh"),
    output:
        seq_lengths=Path(outdir, "seq_lengths.{type}.tsv"),
        stats=Path(outdir, "hybpiper_stats.{type}.tsv"),
    shadow:
        "minimal"
    params:
        parent=lambda wildcards, input: Path(input.sample_files[0]).parent,
    log:
        Path(logdir, "stats.{type}.log"),
    benchmark:
        Path(logdir, "stats.{type}.benchmark.log")
    threads: lambda wildcards, attempt: 16 * attempt
    resources:
        time=lambda wildcards, attempt: 120 * attempt,
    container:
        hybpiper
    shell:
        f"{untar_sample_folders} "
        "hybpiper stats "
        "--targetfile_dna {input.target} "
        "--stats_filename hybpiper_stats.{wildcards.type} "
        "--seq_lengths_filename seq_lengths.{wildcards.type} "
        "{wildcards.type} "
        "{input.namelist} "
        "&> {log} "
        "&& mv hybpiper_stats.{wildcards.type}.tsv {output.stats} "
        "&& mv seq_lengths.{wildcards.type}.tsv {output.seq_lengths} "


# Run hybpiper and compress the results. These will be re-used in subsequent
# workflow runs, if they're present when the workflow starts.
rule pigz:
    input:
        Path(outdir, "assemble", "{sample}.tar"),
    output:
        Path(outdir, "assemble", "{sample}.tar.gz"),
    shadow:
        "minimal"
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
    threads: 4
    container:
        pigz
    shell:
        "pigz -p {threads} -9 > {output} < {input} "


# Hybpiper chokes on empty fastq files so we only add unpaired if it has reads
# in it. At the end, we touch the paralog file. This is required because the
# paralog pipeline won't even run without this file in every sample directory.
# Most real samples have the file and touching it is harmless.  Finally, we
# generate a tar file to deal with the massive number of output files.
rule assemble:
    input:
        r1=Path(read_directory, "{sample}.r1.fastq.gz"),
        r2=Path(read_directory, "{sample}.r2.fastq.gz"),
        unpaired=Path(read_directory, "{sample}.unpaired.fastq.gz"),
        target=fixed_tf,
    output:
        tarfile=temp(Path(outdir, "assemble", "{sample}.tar")),
    shadow:
        "minimal"
    log:
        Path(logdir, "assemble", "{sample}.log"),
    benchmark:
        Path(logdir, "assemble", "benchmark.{sample}.log")
    threads: lambda wildcards, attempt: 16 * attempt
    resources:
        time=lambda wildcards, attempt: 120 * attempt,
        mem_mb=lambda wildcards, attempt: 16e3 * attempt,
    container:
        hybpiper
    shell:
        'unpaired_arg="" '
        "&& "
        'if [ -s {input.unpaired} ]; then unpaired_arg="--unpaired {input.unpaired} "; fi '
        "&& "
        "hybpiper assemble "
        "--readfiles {input.r1} {input.r2} "
        "$unpaired_arg"
        "--targetfile_dna {input.target} "
        "--diamond "
        "--cpu {threads} "
        "--prefix {wildcards.sample} "
        "&> {log} "
        "; "
        "touch "
        "{wildcards.sample}/"
        "{wildcards.sample}"
        "_genes_derived_from_putative_chimeric_stitched_contig.csv"
        "; "
        "tar -cvf {output.tarfile} --directory {wildcards.sample} . "
        "2>> {log} "


# fix the targetfile
rule fix_targetfile:
    input:
        target=tmp_target_file,
        ctl=Path(outdir, "check", "fix_targetfile.ctl"),
    output:
        target_out=fixed_tf,
        report=Path(logdir, "target_file_report.txt"),
    shadow:
        "minimal"
    log:
        Path(logdir, "fix_targetfile.log"),
    benchmark:
        Path(logdir, "fix_targetfile.benchmark.log")
    threads: 1
    resources:
        time=lambda wildcards, attempt: 10 * attempt,
        mem_mb=lambda wildcards, attempt: 1000 * attempt,
    container:
        hybpiper
    shell:
        "hybpiper fix_targetfile "
        "--targetfile_dna {input.target} "
        "--allow_gene_removal "
        "{input.ctl} "
        "&> {log} "
        "&& "
        "mv targetfile_fixed.fasta {output.target_out} "
        "&& "
        "mv fix_targetfile_report.tsv {output.report} "


rule check_targetfile:
    input:
        target=tmp_target_file,
    output:
        ctl=temp(Path(outdir, "check", "fix_targetfile.ctl")),
    shadow:
        "minimal"
    log:
        Path(logdir, "check_targetfile.log"),
    benchmark:
        Path(logdir, "check_targetfile.benchmark.log")
    threads: 1
    resources:
        time=lambda wildcards, attempt: 20 * attempt,
        mem_mb=lambda wildcards, attempt: 1000 * attempt,
    container:
        hybpiper
    shell:
        "hybpiper check_targetfile "
        "--targetfile_dna {input.target} "
        "&> {log} "
        "&& "
        "mv *.ctl {output.ctl} "


# prepare for hybpiper
rule collect_targetfile:
    input:
        target_file,
    output:
        temp(tmp_target_file),
    log:
        Path(logdir, "collect_targetfile.log"),
    container:
        bbmap
    shell:
        "reformat.sh "
        "fixheaders=t "
        "trimreaddescription=t "
        "ignorejunk=f "
        "in={input} "
        "out={output} "
        "2>{log}"


# this is to make sure the namelist ends with a newline
checkpoint collect_namelist:
    input:
        namelist,
    output:
        Path(outdir, "namelist.txt"),
    shell:
        "(cat {input} && echo) > {output}"


# This script will check if the file is a .tar or .tar.gz and expand the
# archives in parallel.
rule generate_untar_script:
    output:
        temp(Path(outdir, "untar.sh")),
    shell:
        """
        cat << 'EOF' > {output}
#!/bin/bash
set -eux
tar_file=\"${{1}}\"
if [[ \"${{tar_file}}\" == *.tar.gz ]]; then
    flags=\"zxf\" 
    base_name=\"$(basename \"${{tar_file}}\" .tar.gz)\"
elif [[ \"${{tar_file}}\" == *.tar ]]; then
    flags=\"xf\" 
    base_name=\"$(basename \"${{tar_file}}\" .tar)\"
fi

mkdir -p \"${{base_name}}\"
tar \"-${{flags}}\" \"${{tar_file}}\" -C \"${{base_name}}\"

EOF

        chmod +x {output} 
        """
