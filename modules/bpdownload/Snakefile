#!/usr/bin/env python3

from snakemake.logging import logger
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
import csv
import tempfile
import functools

#############
# FUNCTIONS #
#############


@functools.cache
def get_dict_of_expt_to_url_from_csvfile(csvfile):
    with open(csvfile, "rt") as f:
        reader = csv.reader(open(csvfile))
        expt_to_url = dict(reader)
    return expt_to_url


def get_expt_url(wildcards):
    """
    Read the URL file AFTER it's been downloaded and parsed.
    This and get_all_expts cause the URL file to be read many times.
    """
    csvfile = checkpoints.parse_run_xml.get(**wildcards).output["csv"]
    expt_to_url = get_dict_of_expt_to_url_from_csvfile(csvfile)
    my_url = expt_to_url[wildcards.expt]
    logger.info(f"Snakemake is checking {my_url}")
    return {
        "url": HTTP.remote(my_url, keep_local=True),
    }


# TODO: The SraRunInfo.txt file has a SINGLE/PAIRED field. Need to deal with
# single files. For now, we are skipping them by checking if row[15]=="PAIRED"
# in get_dict_of_library_to_expt_from_csvfile
@functools.cache
def get_dict_of_library_to_expt_from_csvfile(csvfile):
    LibraryName_to_expt = {}
    with open(csvfile, "r") as f:
        reader = csv.reader(f, delimiter=",")
        # skip the header
        next(reader, None)
        for row in reader:
            library_name = row[0] if row[11] == "" else row[11]
            if row[15] == "PAIRED":
                LibraryName_to_expt[library_name] = row[0]
            else:
                logger.warning(
                    f"SINGLE-end SRR files aren't implemented yet.\n{row[0]} ({row[11]}) will be skipped."
                )
    return LibraryName_to_expt


def get_repair_input(wildcards):
    """
    Map the LibraryName to an expt
    """
    csvfile = checkpoints.get_run_info.get(**wildcards).output["run_info"]
    LibraryName_to_expt = get_dict_of_library_to_expt_from_csvfile(csvfile)
    my_expt = LibraryName_to_expt[wildcards.LibraryName]
    return {
        "r1": Path(run_tmpdir, f"{my_expt}_1.fastq"),
        "r2": Path(run_tmpdir, f"{my_expt}_2.fastq"),
    }


def get_bpdownload_output(wildcards):
    csvfile = checkpoints.get_run_info.get(**wildcards).output["run_info"]
    LibraryName_to_expt = get_dict_of_library_to_expt_from_csvfile(csvfile)
    all_librarynames = sorted(set(LibraryName_to_expt.keys()))
    return expand(
        Path(outdir, "{LibraryName}.r{r}.fastq.gz"),
        LibraryName=all_librarynames,
        r=[1, 2],
    )


###########
# GLOBALS #
###########

# containers
bbmap = "docker://quay.io/biocontainers/bbmap:39.01--h92535d8_1"
entrez = "docker://quay.io/biocontainers/entrez-direct:16.2--he881be0_1"
python = "docker://python:3.10.13"
sratools = "docker://quay.io/biocontainers/sra-tools:3.1.1--h4304569_0"

# set up directories
outdir = Path(config["outdir"] if "outdir" in config else ".")
logger.debug(f"outdir: {outdir}")
logdir = Path(outdir, "logs")

# catch input
accession = config["accession"]
logger.info(f"bpdownload: accession {accession}")

# set up a temporary directory for this run
try:
    run_tmpdir = config["run_tmpdir"]
    logger.info(f"Caught run_tmpdir {run_tmpdir}")
except KeyError as e:
    logger.info(f"{e} not set in config")
    run_tmpdir = tempfile.mkdtemp()
    logger.info(f"Setting run_tmpdir to {run_tmpdir}")
    logger.warning("This probably won't work on a cluster!")

# snakemake's downloader
HTTP = HTTPRemoteProvider()

logger.warning(
    f"Snakemake will check the remote files for {accession}. This can be quite slow."
)

#########
# RULES #
#########


checkpoint target:
    input:
        # This rule is a checkpoint so you don't need to know the names of the
        # samples before you download them. See test-workflows/bpdownload.smk
        # for an example of how to use it.
        get_bpdownload_output,
        Path(outdir, f"SraRunInfo.{accession}.xml"),
        Path(outdir, f"SraRunInfo.{accession}.txt"),
    output:
        touch(Path(outdir, f"bpdownload.{accession}.done")),


rule repair:
    input:
        unpack(get_repair_input),
    output:
        r1=Path(outdir, "{LibraryName}.r1.fastq.gz"),
        r2=Path(outdir, "{LibraryName}.r2.fastq.gz"),
    params:
        zip_threads=lambda wildcards, threads: int(threads // 2),
    log:
        Path(logdir, "repair", "{LibraryName}.log"),
    threads: 4
    resources:
        time=lambda wildcards, attempt: 15 * (2**attempt),
        mem_mb=lambda wildcards, attempt: 4e3 * attempt,
    container:
        bbmap
    shell:
        "repair.sh "
        "-Xmx{resources.mem_mb}m "
        "-Xms100m "
        "in={input.r1} "
        "in2={input.r2} "
        "out={output.r1} "
        "out2={output.r2} "
        "outs=/dev/null "
        "threads={params.zip_threads} "
        "zl=9 "
        "repair=t "
        "tossbrokenreads=t "
        "tossjunk=t "
        "2> {log}"


rule split_srafile:
    input:
        unpack(get_expt_url),
    output:
        r1=Path(run_tmpdir, "{expt}_1.fastq"),
        r2=Path(run_tmpdir, "{expt}_2.fastq"),
    params:
        outdir=lambda wilcdards, output: Path(output.r1).parent,
    log:
        Path(logdir, "split_srafile.{expt}.log"),
    threads: 2
    shadow:
        "minimal"
    container:
        sratools
    shell:
        "ln -s {input.url} ./{wildcards.expt} && "
        "fasterq-dump "
        "--outfile {wildcards.expt} "
        "--threads {threads} "
        "--details "
        "--split-files "
        "--log-level 6 "
        "--verbose "
        "{wildcards.expt} "
        "&> {log} "
        "&& mv {wildcards.expt}* {params.outdir}/ "


checkpoint parse_run_xml:
    input:
        xml=Path(outdir, f"SraRunInfo.{accession}.xml"),
    output:
        csv=Path(run_tmpdir, f"run_to_url.{accession}.txt"),
    container:
        python
    script:
        "scripts/parse_run_xml.py"


# this maps SRR numbers to public URLs
rule get_run_xml:
    input:
        Path(outdir, f"response.{accession}.txt"),
    output:
        Path(outdir, f"SraRunInfo.{accession}.xml"),
    log:
        Path(logdir, f"get_run_xml.{accession}.log"),
    container:
        entrez
    shell:
        "efetch -format native "
        "< {input} "
        "> {output} "
        "2> {log}"


# this maps the SRR numbers to sample names
checkpoint get_run_info:
    input:
        Path(outdir, f"response.{accession}.txt"),
    output:
        run_info=Path(outdir, f"SraRunInfo.{accession}.txt"),
    log:
        Path(logdir, f"get_run_info.{accession}.log"),
    container:
        entrez
    shell:
        "efetch -format runinfo "
        "< {input} "
        "> {output} "
        "2> {log}"


rule search_by_accession:
    output:
        temp(Path(outdir, f"response.{accession}.txt")),
    params:
        accession=accession,
    log:
        Path(logdir, f"search_by_accession.{accession}.log"),
    container:
        entrez
    shell:
        "esearch "
        "-db sra "
        "-query {params.accession}[bioproject] "
        "> {output} "
        "2> {log}"


onsuccess:
    logger.debug(
        f"""
get_dict_of_expt_to_url_from_csvfile: {get_dict_of_expt_to_url_from_csvfile.cache_info()}
get_dict_of_library_to_expt_from_csvfile: {get_dict_of_library_to_expt_from_csvfile.cache_info()}
"""
    )


onerror:
    logger.debug(
        f"""
get_dict_of_expt_to_url_from_csvfile: {get_dict_of_expt_to_url_from_csvfile.cache_info()}
get_dict_of_library_to_expt_from_csvfile: {get_dict_of_library_to_expt_from_csvfile.cache_info()}
"""
    )
